---
title: "Week 10 Assignment – Sentiment Analysis"
output: html_document
---

### Load Libraries

Re-create and analyze primary code from the textbook.
Provide citation to text book, using a standard citation syntax like APA or MLA.

```{r}
library(tidytext)
library(dplyr)
```
### The sentiments datasets:

The tidytext package provides access to several sentiment lexicons. 
The three general purpose lexions that are based on unigrams, i.e., single words are:
1. AFINN - lexicon assigns words with a score that runs between -5 and 5,
            with negative scores indicating negative sentiment and positive scores indicating positive sentiment.
2. nrc - lexicon categorizes words in a binary fashion (“yes”/“no”) into categories of positive, negative.
3. bing - lexicon categorizes words in a binary fashion into positive and negative categories.

The function get_sentiments() allows us to get specific sentiment lexicons with the appropriate measures for each one.

```{r}
get_sentiments("afinn")
```

```{r}
get_sentiments("bing")
```

```{r}
get_sentiments("nrc")
```

### Sentiment analysis with inner join:


```{r}
library(janeaustenr)
library(dplyr)
library(stringr)

tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>% ungroup() %>%

  unnest_tokens(word, text)
```

1. We chose the name 'word' for the output column from unnest_tokens() function,
   because the sentiment lexicons and stop word datasets have columns named word; 
   performing inner joins and anti-joins is thus easier.

2. The text is in a tidy format with one word per row, we can perform sentiment analysis now.

3. First, use the NRC lexicon and filter() for the joy words. 

4. Then, filter() the data frame with the text from the books for the words from Emma and then use    inner_join() to perform the sentiment analysis

5. Use count() from dplyr, to count the most common joy words in Emma.

```{r}

nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```
There are mostly positive, happy words. 

### Now, examine how sentiment changes throughout each novel,
### using mostly dplyr functions.

1. First,find a sentiment score for each word using the Bing lexicon and inner_join().

2. Then, count up how many positive and negative words there are in defined sections of each book.

3. Next, define index to keeps track of which 80-line section of text we are 
   counting up negative and positive sentiment in.
   The %/% operator does integer division (x %/% y is equivalent to floor(x/y))
   
4. Use pivot_wider() so that we have negative and positive sentiment in separate columns.

5. Lastly, calculate a net sentiment (positive - negative)

```{r}
library(tidyr)

jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

Now, plot these sentiment scores across the plot trajectory of each novel,
by plotting against the index on the x-axis that keeps track of narrative 
time in sections of text.

```{r}
library(ggplot2)

ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

### Comparing the three sentiment dictionaries:

Now, compare all three sentiment lexicons and examine how the sentiment 
changes across the narrative arc of Pride and Prejudice.

1. First, use filter() to choose only the words from the one novel we are interested in.

```{r}
pride_prejudice <- tidy_books %>% 
  filter(book == "Pride & Prejudice")

pride_prejudice
```
Since, AFINN lexicon measures sentiment with a numeric score between -5 and 5,
now, use a different pattern for the AFINN lexicon than for the other two.

2. Use inner_join() to calculate the sentiment in different ways.

3. Next, define index to keeps track of which 80-line section of text we are 
   counting up negative and positive sentiment in.
   The %/% operator does integer division (x %/% y is equivalent to floor(x/y))
   
4. Use pivot_wider() so that we have negative and positive sentiment in separate columns.

5. Lastly, calculate a net sentiment (positive - negative)

```{r}
afinn <- pride_prejudice %>% 
  inner_join(get_sentiments("afinn")) %>% 
  group_by(index = linenumber %/% 80) %>% 
  summarise(sentiment = sum(value)) %>% 
  mutate(method = "AFINN")

bing_and_nrc <- bind_rows(
  pride_prejudice %>% 
    inner_join(get_sentiments("bing")) %>%
    mutate(method = "Bing et al."),
  pride_prejudice %>% 
    inner_join(get_sentiments("nrc") %>% 
                 filter(sentiment %in% c("positive", 
                                         "negative"))
    ) %>%
    mutate(method = "NRC")) %>%
  count(method, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment,
              values_from = n,
              values_fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

Now, we have an estimate of the net sentiment (positive - negative) in each chunk 
of the novel text for each sentiment lexicon. 
Let’s bind them together and visualize.

```{r}
bind_rows(afinn, 
          bing_and_nrc) %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")
```

