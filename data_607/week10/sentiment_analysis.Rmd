---
title: "Week 10 Assignment – Sentiment Analysis"
output: html_document
---

### Downloading and installing tm the package from CRAN

use an R package called “tm”.This package supports all text mining functions like loading data,cleaning data and building a term matrix.
```{r}

#install.packages("tm")

```

### Import Libraries

```{r}
library(dplyr)
library(tidyverse)
library(textreadr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(tm)
```

### Loading Data

Text to be mined is be loaded into R.It comes from text files(.txt).
To be used in the tm package, it is turned into a “corpus”.
The tm package use the Corpus() function to create a corpus.

```{r}
#loading a text file from github

github_link <- "https://raw.githubusercontent.com/uzmabb182/CUNY-SPS-Assignments/main/data_607/week10/weather-tweet.csv"

weather_data<- read.csv(github_link)
weather_data
```

### Loading data into a vector

```{r}
data_vector <- weather_data$tweet_text                         
#data_vector
```

### Loading data as corpus

```{r}
#VectorSource() creates character vectors
mydata <- Corpus(VectorSource(data_vector))

mydata
```

### Data Cleaning Steps

```{r}
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
```

```{r}
#remove ������ what would be emojis
mydata<-tm_map(mydata, content_transformer(gsub), pattern="\\W",replace=" ")
```

```{r}
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL)
)
```

```{r}
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
```

```{r}
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
```

```{r}
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
```

```{r}
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)

```

```{r}
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
```

### Stemming Process using SnowballC library.
Stemming is the process of gathering words of similar origin into one word for example “communication”, “communicates”, “communicate”. Stemming helps us increase accuracy in our mined text by removing suffixes and reducing words to their basic forms.

```{r}
library(SnowballC)
mydata <- tm_map(mydata, stemDocument)
mydata
```

### Creating a term Matrix and finding word frequencies:

The matrix logs the number of times the term appears in our clean data set thus being called a term matrix.

```{r}
#create a term matrix and store it as dtm
dtm <- TermDocumentMatrix(mydata)
dtm
```

### Analysizing using tidytool

we need to turn it into a one-term-per-document-per-row data frame first.
```{r}

library(dplyr)
library(tidytext)

tweet_td <- tidy(dtm)

tweet_td
```

```{r}

```

