---
title: "Week 10 Assignment – Sentiment Analysis"
output: html_document
---

### Downloading and installing tm the package from CRAN

use an R package called “tm”.This package supports all text mining functions like loading data,cleaning data and building a term matrix.
```{r}

#install.packages("tm")

```

### Import Libraries

```{r}
library(dplyr)
library(tidyverse)
library(textreadr)
library(tidytext)
library(tidyr)
library(ggplot2)
library(tm)
```

### Loading Data

Text to be mined is be loaded into R.It comes from text files(.txt).
To be used in the tm package, it is turned into a “corpus”.
The tm package use the Corpus() function to create a corpus.

```{r}
#loading a text file from github

github_link <- "https://raw.githubusercontent.com/uzmabb182/CUNY-SPS-Assignments/main/data_607/week10/weather-tweet.csv"

weather_data<- read.csv(github_link)
weather_data
```

### Loading data into a vector

```{r}
data_vector <- weather_data$tweet_text                         
#data_vector
```

### Loading data as corpus

```{r}
#VectorSource() creates character vectors
mydata <- Corpus(VectorSource(data_vector))

mydata
```

### Data Cleaning Steps

```{r}
# convert to lower case
mydata <- tm_map(mydata, content_transformer(tolower))
```

```{r}
#remove ������ what would be emojis
mydata<-tm_map(mydata, content_transformer(gsub), pattern="\\W",replace=" ")
```

```{r}
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeURL)
)
```

```{r}
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
mydata <- tm_map(mydata, content_transformer(removeNumPunct))
```

```{r}
# remove stopwords
mydata <- tm_map(mydata, removeWords, stopwords("english"))
```

```{r}
# remove extra whitespace
mydata <- tm_map(mydata, stripWhitespace)
```

```{r}
# Remove numbers
mydata <- tm_map(mydata, removeNumbers)

```

```{r}
# Remove punctuations
mydata <- tm_map(mydata, removePunctuation)
```

### Stemming Process using SnowballC library.
Stemming is the process of gathering words of similar origin into one word for example “communication”, “communicates”, “communicate”. Stemming helps us increase accuracy in our mined text by removing suffixes and reducing words to their basic forms.

```{r}
library(SnowballC)
mydata <- tm_map(mydata, stemDocument)
mydata
```

### Creating a term Matrix and finding word frequencies:

The matrix logs the number of times the term appears in our clean data set thus being called a term matrix.

```{r}
#create a term matrix and store it as dtm
dtm <- TermDocumentMatrix(mydata)
dtm
```

### Analysizing using tidytool

we need to turn it into a one-term-per-document-per-row data frame first.
```{r}

library(dplyr)
library(tidytext)

tweet_td <- tidy(dtm)

tweet_td
```
### Performing Sentiment Analysis:

Performing sentiment analysis now using the lexicon from Bing Liu and collaborators, which assigns positive/negative labels for each word

```{r}
ap_sentiments <- ap_td %>%
  inner_join(get_sentiments("bing"), by = c(term = "word"))

ap_sentiments
```

We can find the most negative documents:

```{r}
library(tidyr)

ap_sentiments %>%
  count(document, sentiment, wt = count) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative) %>%
  arrange(sentiment)
```

### Visualization Analysis

visualize which words contributed to positive and negative sentiment:
Showing more negative word frequencies than positive
```{r}
library(ggplot2)

ap_sentiments %>%
  count(sentiment, term, wt = count) %>%
  filter(n >= 5) %>%
  mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
  mutate(term = reorder(term, n)) %>%
  ggplot(aes(term, n, fill = sentiment)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ylab("Contribution to sentiment")
```

### Steps to Create Word Clouds:

Most often, word clouds are used to analyse twitter data or a corpus of text.
Using the TermDocumentMatrix tdm created above which contains each word in your 
first column and their frequency in the second column.

```{r}
library(wordcloud)

matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)
df
```

### Generate the word cloud:

```{r}
set.seed(1234) # for reproducibility 

wordcloud(words = df$word, freq = df$freq, min.freq = 1,
          max.words=200, random.order=FALSE, rot.per=0.35,
          colors=brewer.pal(8, "Dark2"))
```

